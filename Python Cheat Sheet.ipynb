{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Cheat Sheet\n",
    "\n",
    "This notebook contains useful python code, particularly for data analytics purposes\n",
    "- by: Daniel Bejarano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Packages and Libraries\n",
    "\n",
    "### 0.1 Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Need to install with pip or conda:\n",
    "    - conda install sqlalchemy\n",
    "    - conda install lxml\n",
    "    - conda install html5lib\n",
    "    - conda install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Command Prompt\n",
    "\n",
    "- To navigate directories forward, use \"cd\" name_name_of_next_path\n",
    "- To navigate directories backward use \"cd..\"\n",
    "- To install packagages go to: \"C:\\Users\\dbejarano\\Anaconda2\\Scripts>\" easy_install ______\n",
    "- Check python version - go to Anaconda3 folder and type \"python --version\"\n",
    "    - \"python -- version\" will check in Python 2\n",
    "    - \"python3 --version\" will check in Python 3\n",
    "- Create virtual environment with another python version\n",
    "    - \"conda create -n py35 python=3.5.2\"\n",
    "    - \"activate py35\"\n",
    "- \"pwd\" checks location of jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Resources:\n",
    "\n",
    "- Source for navigating environments:\n",
    "    - https://medium.freecodecamp.org/why-you-need-python-environments-and-how-to-manage-them-with-conda-85f155f4353c\n",
    "- Jupyter Markdowns:\n",
    "    - https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Jupyter Shortcuts\n",
    "- Press \"Shift + Tab\" after a function or object to get information on it\n",
    "- Press \"Tab\" after an object to see what methods are available for that object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading and Exporting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Setting and Navigating Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_folder = os.path.dirname(os.path.abspath(\"__file__\")) \n",
    "file = os.path.join(this_folder, 'name_of_file.txt')\n",
    "\n",
    "# Searching through files in folder\n",
    "for fname in os.listdir(dir_name):\n",
    "    if BLAHBLAH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text files\n",
    "file_data = open(twitter_access, 'r')\n",
    "file_date.close()\n",
    "# or...\n",
    "with open('file_name.txt', 'r') as file:\n",
    "    \n",
    "# Flat Files\n",
    "file = 'file_name.txt'\n",
    "array = np.loadtxt(file, delimiter='\\t', skiprows=1, dtype=float, usecols=[0,3])\n",
    "\n",
    "# DataFrames\n",
    "df = pd.read_csv(file, sep='\\t', comment='#', na_values='Nothing') \n",
    "    # comment is the character after which there are added comments we may not want; \n",
    "    # na_values are the value or string we are telling pd to take as NAs\n",
    "    # sep means that the values are separated with a tab\n",
    "    \n",
    "df.read_excel(file='file_name.xlsx', sheetname='Sheet_name')\n",
    "\n",
    "data = pd.read_html('website_name')\n",
    "data[0] \n",
    "    # read_html reads all the file and creates several df, iterate through them to get what youwant\n",
    "\n",
    "# Read from SQL\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///:memory:') # creates a temporary engine with a dataframe\n",
    "df.to_sql('my_table', engine)\n",
    "''' pandas by itself is not the best tool to read directly from SQL. It's recommmended to read \n",
    "using another tool and then convert to a dataframe'''\n",
    "sqldf = pd.read_sql('my_table', con=engine)\n",
    "\n",
    "# Importing Images\n",
    "from PIL import Image\n",
    "jpgfile = Image.open(\"picture.jpg\")\n",
    "print(jpgfile.bits, jpgfile.size, jpgfile.format)\n",
    "\n",
    "# MAT files - used a lot with Coursera's ML class\n",
    "from scipy.io import loadmat\n",
    "data = loadmat('file_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Writing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('My_output', index=False) \n",
    "df.to_excel(file_name, sheet_name='desired_sheet_name') \n",
    "    # there are many 'to' options: to_json, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 The Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Daniel Bejarano'\n",
    "print('hello %s' % name)\n",
    "\n",
    "# To split strings into a list of its components\n",
    "my_string.split() # splits on words\n",
    "my_string.split('#') # will split string at '#'\n",
    "\n",
    "my_string[::-1] # function to reverse strings\n",
    "\n",
    "# To join a list of words into a string\n",
    "' '.join(my_list) #join leaving a space in between\n",
    "';'.join(my_list) #join leaving a \";\" in between\n",
    "\n",
    "# Variables within strings\n",
    "num=12\n",
    "name='Daniel'\n",
    "\"My number is {} and my name is {}\".format(num, name) # or use...\n",
    "\"My number is {one} and my name is {two}\".format(one=num, two=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A backslash in preceeding a character gives it special meaning, BUT in Reg Exp it does the opposite\n",
    "\\n # is a new line\n",
    "r'\\$' # removes the special meaning of the dollar sign inside regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lists, Tuples & Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(1, 10)) # Creates list from 1 to 10\n",
    "\n",
    "# List Comprehension\n",
    "x = [1,2,3,4,5]\n",
    "out = [num**2 for num in x]\n",
    "\n",
    "# Pop-out elements from a list\n",
    "first = x.pop(0) # first will be equal to 1 and x will now be equal to [2,3,4,5]\n",
    "\n",
    "# Tuple Unpacking\n",
    "tupl = [(1,2), (3,4), (5,6)]\n",
    "for a,b in tupl:\n",
    "    print(a)\n",
    "    \n",
    "# Fast way to create a list of strings\n",
    "my_list = 'ARG VEN USA MEX AUS'.split()\n",
    "\n",
    "# SETS\n",
    "set1 = {1, 1, 2, 3, 4, 5, 5, 5, 6} # would print only {1,2,3,4,5,6} since sets only keeps unique elements\n",
    "\n",
    "# Dictionaries\n",
    "my_dict.keys()\n",
    "my_dict.values()\n",
    "my_dict.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''When arrays are behaving stupidly, try reshaping so they have two dimensions (204, 1) instead\n",
    "of just (204, ). Then try to flatten if that doesn't work: my_array.flatten()'''\n",
    "\n",
    "lst = [1,2,3]\n",
    "arr = np.array(lst)\n",
    "\n",
    "# To create an array\n",
    "np.arange(0, 11, 2) # from 0 to 11 by twos (stepsize)\n",
    "np.zeros(3) # 1-D array of 3 zeros\n",
    "np.ones((3,4)) # a 3 by 4 matrix of ones\n",
    "np.linspace(0, 5, 10) # vector of 10 evenly spaced points from 0 to 5 \n",
    "np.eye(4) # identify matrix of size 4\n",
    "np.empty(5) # creates an empty array with values close to, but not zero\n",
    "\n",
    "# Random Arrays\n",
    "np.random.rand(3,3) # 3 by 3 matrix of random numbers from a UNIFORM distribution\n",
    "np.random.randn(2) # 2 random numbers from a NORMAL distribution\n",
    "np.random.randint(1, 100, 4) # 4 random integers from 1 to 99\n",
    "\n",
    "# Random Shuffle\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Random Shuffle two arrays the same way (when X and y are not in the same array)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# Method of Arrays\n",
    "arr.reshape(5,5) # Reshape a list into a 5x5 matrix\n",
    "arr.max() # returns the maximum value\n",
    "arr.argmax() # returns the index of the maximum value\n",
    "arr.dtype() # type of data\n",
    "arr.shape # gives the shape of the array\n",
    "arr.sum(axis=0) # sums over all columns in an array\n",
    "\n",
    "# Indexing Arrays\n",
    "arr[3:, 3] # will return rows from 3 to end and column 4. However, it will return as a vector\n",
    "arr[3:, 2:4] # will return the same as above, but as a 2D array\n",
    "\n",
    "# Operations on Arrays\n",
    "np.max(arr)\n",
    "np.sqrt(arr)\n",
    "np.sin(arr) # etc...\n",
    "\n",
    "# Propagation of Arrays\n",
    "arr = [0,1,2,3,4,5,6,7,8]\n",
    "slice_of_arr = arr[:4] # getting a section of the array does not make a copy. It is just a view...\n",
    "slice_of_arr = 99 # when we set the values in the slice to 99, it will change them in the slice...\n",
    "arr # and since the slice is just a view, it will also change them in the original array. \n",
    "# To make a copy\n",
    "arr_copy = arr.copy()\n",
    "\n",
    "# 2D Arrays\n",
    "arr_2d = np.array([[1,2,3], [5,6,7], [8,9,10]])\n",
    "arr_2d[2,1] # is a better way of indexing than arr_2d[2][1]\n",
    "\n",
    "# To get certain parts of the array based on a condition\n",
    "array[array>5] # [array>5] returns a list of booleans. array[list_of_booleans] returns the \"True\" values\n",
    "\n",
    "# Count Number of Occurrences on an Array\n",
    "import collections, numpy\n",
    "collections.Counter(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Matrix Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(X, theta)\n",
    "np.hstack((ones, X)) # to add a column of 1s to a table X\n",
    "X.T[1]\n",
    "X[1] # if this example gets row 1, the previous one gets column 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Series\n",
    "pd.Series(data=some_list, index=some_labels)\n",
    "pd.Series(data=some_array, index=some_labels) # same as above\n",
    "pd.Series([32, 45, 32, 34], ['USA', 'MEX', 'GER', 'AUS']) # same as above\n",
    "pd.Series(some_dictionary) # it will get the data and labels automatically from the dictionary\n",
    "\n",
    "# Indexing on Series\n",
    "ser[name_of_row_label]\n",
    "\n",
    "# Operations with series\n",
    "ser1 + ser2 \n",
    "    # will sum based on the index label. Ex: 'USA' on str1 will be added to 'USA' on str2\n",
    "    # labels that are not in both series will have a value of NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from scratch\n",
    "df = pd.DataFrame(np.random.randn(5,4), ['A', 'B', 'C', 'D', 'E'], ['W', 'X', 'Y', 'Z'])\n",
    "\n",
    "# To display dataframes instead of printing them (so they look like dataframes)\n",
    "display(df)\n",
    "\n",
    "# Adding and Removing column\n",
    "df['new'] = df['W'] + df['X']\n",
    "df.drop['new', axis=1, inplace=True] # if inplace is False then it's not dropped from the original df\n",
    "\n",
    "# Select Information based on condition\n",
    "my_df[my_df['column'] == 'row_value'] ### to get all rows whose value equals 'row_value' under 'column'\n",
    "df[df['W']>0][['X', 'Y']] # returns values in columns X and Y for rows in which W >0 \n",
    "df[(df['W']>0) & (df['Y']<1)] # two conditions\n",
    "df[(df['W']>0) | (df['Y']<1)] # one condition OR another. \n",
    "    '''Python can't use 'and' and 'or' when given multiple values. It just does one to one'''\n",
    "df.idxmax() # gives back the index of the maximum value\n",
    "subset = df[df['Column'].isin(other_df['Column'])] \n",
    "    # subset of rows from a dataframe if they match values of another dataframe\n",
    "\n",
    "# Resetting Index\n",
    "df.reset_index(inplace=False) # makes the index into a column\n",
    "df.set_index('column you want to become new index') # to set the index to values in a column\n",
    "\n",
    "# Extracting Info from DataFrames\n",
    "my_df['column'] #returns a Pandas series\n",
    "my_df[['column']] #returns a Pandas dataframe\n",
    "my_df[0:5] # returns the first five rows\n",
    "my_df.iloc[0:5] # is equivalent to the above example\n",
    "my_df.loc['row'] #returns a pandas series \n",
    "    # loc is labeled based. iloc is integer (position) based\n",
    "my_df.loc[['row']] # returns the same as above but as a dataframe\n",
    "my_df.loc[4] #returns a pandas series \n",
    "my_df.loc[[4]] # returns the same as above but as a dataframe\n",
    "my_df.loc[['row1', 'row2']] # same as previously but for several rows\n",
    "my_df.loc[[4, 1]] # same as previously but for several rows\n",
    "my_df.loc['row', 'column']\n",
    "my_df.loc[['row1', 'row2'], ['column1', 'column2']] # returns those rows for that column as a dataframe\n",
    "my_df.iloc[[3, 4], [0, 1]] # returns the 3rd and 4th rows for the zeroth and first columns\n",
    "\n",
    "# Change Info in DataFrames\n",
    "df['col']['row label'] = new_value \n",
    "    '''using loc or iloc only extracts values, doesn't change in place'''\n",
    "df['col_name'] = df['col_name'].astype(int) # changes data type\n",
    "df['col_name'] = df['col_name'].astype(str).astype(int) \n",
    "    # sometimes you have to convert to str first to then convert to int\n",
    "df.replace(['value1', 'value2'], ['r_value1', 'r_value2'], inplace=True) # replaces data\n",
    "    \n",
    "# Mapping\n",
    "column_cat = (df['column'] > some_value).map({False:0, True:1}) \n",
    "    # Maps columns into categorical variables based on condition\n",
    "# Example of mapping numbers to days of week\n",
    "dmap = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'} \n",
    "df['Day of Week'] = (df['Day of Week']).map(dmap)\n",
    "\n",
    "# Groupby\n",
    "df.groupby('column').count() # groups rows based on euql values under the column called (if they are all \"USA\" or instance)\n",
    "df.groupby('column').sum().loc['value'] # gives the sum of all rows with value = 'value' under 'column'\n",
    "df.groupby('column').describe() # gives several different values for each category in that column\n",
    "df.grouby('col1')['col2'].mean() # if we were interested on the mean of col2, grouped by col1\n",
    "# Example to get average salary for every year in the datasetc\n",
    "sal.groupby('Year').mean()['BasePay']\n",
    "# Example: Restructure data to create matrix with Hours as columns and DayofWeek as rows\n",
    "dayhour = df.groupby(by=['Day of Week', 'Hour']).count()['any_column'].unstack()\n",
    "\n",
    "# Mult-index DataFrame\n",
    "outside  = ['G1', 'G1', 'G1', 'G2', 'G2', 'G2']\n",
    "inside = [1,2,3,1,2,3]\n",
    "hier_index = list(zip(outside, inside))\n",
    "hier_index = pd.MultiIndex.from_tuples(hier_index)\n",
    "df. DataFrame(rand(6, 2), hier_index, ['A', 'B']) # where rand was gotten from 'from numpy.random import rand'\n",
    "df.loc['G1'].loc[1] # index on the outside index then on the inside index\n",
    "df.index.names = ['Groups', 'Num'] # labels both indexes\n",
    "df.xs(1, level='Num') # cross-section indexing, usefull in multi-index dataframes\n",
    "\n",
    "# Missing Data\n",
    "df.dropna(axis=0) # drops all rows (or columns if axis=1) with at leats one NaN\n",
    "df.dropna(thresh=10, axis=0) # drops rows/columns that have at least 10 NaN\n",
    "df['A'].fillna(value=df['A'].mean()) # fills NaN with the mean of values in column A\n",
    "df.drop('col', axis=1, inplace=True)\n",
    "df.drop_duplicated() # Remove duplicated values, df.duplicates() lets you see them\n",
    "\n",
    "# Join, Merge & Concatenate\n",
    "pd.concat([df1, df2, df3], axis=0) # will add df2 and df3 rows to df1, change to axis=1 for columns\n",
    "pd.merge(left_df, right_df, how='inner', on='name_of_key_column_that dfs share') #analogous to vlookup\n",
    "left_df.join(right_df) # for joining via the index\n",
    "\n",
    "# Operators\n",
    "df['col1'].unique() # gives back the set of unique values on that column\n",
    "df['col1'].nunique() # number of unique values on that column\n",
    "df['col1'].value_counts() # set of unique values along with the number of times each appears\n",
    "df['col1'].apply(len) # applies a function to all the values in that column, in this case 'len'\n",
    "df['col1'].apply(lambda x: x*2) # applies anonymous function\n",
    "df.drop('col1', axis=1, inplace=False) # drops a column or row\n",
    "df.columns # gives you back the list of column names\n",
    "df.index # gives back the list of index names\n",
    "df.sort_values(by='col1', axis=0) # sorts df by the column specified (index remains with original values)\n",
    "df.isnull() # gives back booleans of whether the value is null or not\n",
    "df.pivot_table(values='col1', index=['col2', 'col3'], columns=['col4']) \n",
    "    # creates pivot table containing the values on col1, with col2 and col3 forming a multi-index and the \n",
    "    # values of col4 providing the columns\n",
    "pd.to_datetime(df['col']) # converts a string in the form of datetime to datetime\n",
    "df['column_name'].value_counts() # count number of occurrences\n",
    "df.dtypes # gets data type\n",
    "type(df['col'][0]) # data type to a granular level into the object\n",
    "xy_train_0count = len(xy_train) - xy_train.astype(bool).sum(axis=0) # count the number of zeros on each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function\n",
    "def my_func(name='Default Name'):\n",
    "    print('Name is' + name)\n",
    "    \n",
    "my_func() # executes the function\n",
    "my_func # gives you back the function\n",
    "\n",
    "# MAP - Creates an iterator over a function\n",
    "seq = ['1','2','3','4']\n",
    "list(map(my_func, seq))\n",
    "\n",
    "# LAMBDA - Anonymous function for quick/easy function creation\n",
    "x = (lambda var: var*2)\n",
    "print(x(3))\n",
    "\n",
    "seq = [1,2,3,4,5]\n",
    "list(map(lambda num: num*3, seq))\n",
    "\n",
    "# FILTER - Analogous to \"map\" but it filters out the input you give it\n",
    "list(filter(lambda num:num%2==0, seq)) # Filter takes a boolean as input\n",
    "\n",
    "# Use assert to check  that a criteria is met and hault the operation otherwise\n",
    "def tag(word):\n",
    "    assert isinstance(word, basestring), \"argument to tag() must be a string\"\n",
    "    if word in my_list:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return \"nou\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Math Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import pi # Python math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conditional Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = np.sum((data['Decision']==0) & (data['Predictions'] ==0))\n",
    "\n",
    "# Change array to 1s and 0s if condition is met\n",
    "ones_and_zeros = (some_array == 'some_value').astype(int)\n",
    "\n",
    "# Sum of conditions when values are 1 or 0s\n",
    "TP = np.sum(data['Decision']*data['Predictions']) # using numerical cleverness\n",
    "TN = np.sum((data['Decision']==0) & (data['Predictions'] ==0)) #using if and cleverness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. For and While Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE-LINE for LOOPS\n",
    "\n",
    "# flattening a list of lists using coditionals\n",
    "flatten_list = [y for x in l_o_lists for y in x if y not in some_list] # t=token, c=comment\n",
    "\n",
    "# perform a function on a list of items\n",
    "result = [lemmatizer.lemmatize(t) for t in items]\n",
    "\n",
    "# To iterate over all combination of values in multiple lists\n",
    "import itertools\n",
    "for x in itertools.product(list1, list2):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1 Graphs and Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.1.1 Directly From Pands - Useful for making quick plots, but can't be personalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'].hist()\n",
    "df['col'].plot.hist(figsize=(12,3))\n",
    "df.plot.area(alpha=0.4) # transparent area plot\n",
    "df.plot.bar(stacked=False)\n",
    "df.plot.line(x=df['col1'].index, y='col2'])\n",
    "df.plot.scatter(x='col1', y='col2', c='col3', s=df['col4']) \n",
    "    # makes the color off a third column, and size of dots off a 4th column. 4 levels of info\n",
    "df[['col1', 'col2']].plot.box() # compares values from 2 columns\n",
    "df.plot.hexbin(x='col1', y='col2', gridsize=15) \n",
    "df.plot.density()\n",
    "df.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.1.2 Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: matplotlib.org\n",
    "\n",
    "# Setup\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.show() # ONLY if not using Jupyter notebooks\n",
    "\n",
    "# Functional plots\n",
    "plt.plot(x, y)\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(label)\n",
    "plt.ylabel(label)\n",
    "plt.title(title)\n",
    "\n",
    "# Object-Oriented Method - Allows for Multiplots\n",
    "fig = plt.figure(figsize=(12, 8), dpi=100) # created plot as an object - blank canvas. Dots per inch\n",
    "ax1 = fig.add_axes([0, 0, 1, 1]) # adds axes per:[left, bottom, width, height]\n",
    "ax2 = fig.add_axes([0.2, 0.5, 0.4, 0.3]) # like axes1 above, it's in relation to a blank canvas\n",
    "ax1.set_xlabel(label) # same for ylabel and title\n",
    "ax1.set_xlim([0,1]) # sets the lower and upper bound limit for x-axis. Same for y\n",
    "ax.xaxis.set_ticklabels(['business', 'health']); ax.yaxis.set_ticklabels(['health', 'business']);\n",
    "\n",
    "# Plotting Details\n",
    "ax1.plot(x, y, label='x and y', color='b', lw=1, alpha=0.6, ls='--', marker='o', markersize=5) \n",
    "# plots based on ax1. Label is used when ax.legend() gets called (below)\n",
    "'''You can substitute the color with RBG Hex Codes like #FFC00. Alpha controls transparency. \n",
    "ls and lw are linestyle and linewidth. Marker shows the points'''\n",
    "\n",
    "# Multiplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, current) # Create several plots with their respective axes\n",
    "axes[0].plot(x, y)\n",
    "axes[1].plot(x, z)\n",
    "axes[0].set_title('first plot')\n",
    "plt.tight_layout() # spaces subplots so they don't overlap\n",
    "\n",
    "# Save Plot\n",
    "fig.savefig('my_figure.png', dpi=200) # saves figure\n",
    "\n",
    "# Add legends\n",
    "ax.legend(loc=0) # plots at 'best' location. You can replace '0' with coordinates (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.1.3 Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "'''Color Map Ref: https://matplotlib.org/examples/color/colormaps_reference.html'''\n",
    "plt.title('title') # is also used for sns plots\n",
    "\n",
    "# To Set figure size\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "sns..... #whatever plot\n",
    "\n",
    "# To Relocate Legend\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n",
    "\n",
    "# Multiplot\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "sns.heatmap(data1, ax=ax1)\n",
    "sns.heatmap(data2, ax=ax2)\n",
    "plt.show()\n",
    "\n",
    "# Distribution Plots\n",
    "sns.distplot(df['col'], kde=True, bins=30) # for univariate data\n",
    "sns.jointplot(x='col1', y='col2', data=df, kind='scatter') # for bi-variate data\n",
    "sns.pairplot(df, hue='cat-col', palette='') # pair-wise relations for numerical columns across entire df\n",
    "'''KDE Plots - Kernel Density Estimation: KDE plots imposes a normal distribution on \n",
    "top of every observation, then sums the distributions'''\n",
    "sns.rugplot(df['col'], ) # draws a dash for every data point\n",
    "\n",
    "# Categorical Plots\n",
    "sns.barplot(x='cat_col1', y='col2', data=df, estimator=np.std) # shows a \"grouby\" representation of data\n",
    "sns.countplot(x='cat_col', data=df) # like barplot, but y is shown as a count 'estimator'\n",
    "sns.boxplot(x='cat_col1', y='col2', data=df, hue='cat_col3')\n",
    "sns.violinplot(x='day', y='total_bill', data=tips, hue='cat_col3', split=True) \n",
    "    # shows the KDE of the underlying distribution\n",
    "sns.stripplot(x='cat_col1', y='col2', data=df, jitter=True) # can use hue and split as above\n",
    "sns.swarmplot(x='cat_col1', y='col2', data=df) # combination of strip and violin plots\n",
    "'''You can combine plots: run violin and str with the same data one right after the other'''\n",
    "sns.factorplot(x='cat_col1', y='col2', data=df, kind='bar') \n",
    "    # General method to call all of the above by changing the 'kind'\n",
    "\n",
    "# Matrix Plots\n",
    "''' Need to have data in matrix form. Example: df.corr() or df.pivot_table() '''\n",
    "sns.heatmap(df_matrix, annot=True, cmap='coolwarm', ax=ax, linecolor='b', linewidth=1); \n",
    "sns.clustermap(df) # clusters columns and rows together based on their level of similarity\n",
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis') \n",
    "    # great visualization of missing (NA, NaN, null) values\n",
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis') \n",
    "    # great visualization of missing (NA, NaN, null) values\n",
    "\n",
    "# Grid Mechanism\n",
    "g = sns.PairGrid(df) # gives you an empty set of canvases\n",
    "g.map(plt.scatter) # plots a scatterpoint at each of the canvases. \n",
    "# but it gets better...\n",
    "g.map_diag(sns.distplot) # specifies plot at diagonals\n",
    "g.map_upper(plt.scatter) # specifies plots at top triangle\n",
    "g.map_lower(plt.kdeplot) # specifies plots at bottom triangle\n",
    "# ------- #\n",
    "g = sns.FacetGrid(data=df, col='cat_col1', row='cat_col2', size=6, aspect=2) # Great customization!\n",
    "g.map(sns.distplot, 'col1') # can pass a scatter plot (2 arguments) but needs a second column 'col2'\n",
    "\n",
    "# Regression Plots\n",
    "sns.lmplot(x='col1', y='col2', data=df, scatter_kws={'s':100}, col='cat_col1', row='cat_col2') \n",
    "# scatter w/ line and big marker size. col and row separate the plot into several, based on the categorical variable\n",
    "'''Plot Scatter with its regression line:'''\n",
    "sns.regplot(x='col1', y='col2', fit_reg=False) \n",
    "    # set to false so we can estimate based on our model prediction instead, in case\n",
    "sns.lineplot(x='col1', y=predictions)# we want a non-linear fit\n",
    "\n",
    "# Style and Colors\n",
    "sns.set_palette('GnBu_d')\n",
    "sns.set_style('darkgrid') # sets the overal style of the plot\n",
    "plt.style.use('ggplot')\n",
    "sns.despine() # removes spines. By default only top and right spines\n",
    "plt.figure(figsize=(12, 3)) # can be used before sns to set the size of the plot\n",
    "sns.set_context('poster', font_scale=1) # the 1st arg changes the size of it all. Font_scale changes font"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.1.4 Plotly and Cufflinks - Interactive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: plotly/python/references\n",
    "# Cufflinks is the library that connects Plotly with Pandas\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflink as cf\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "#------#\n",
    "df.iplot(kind='scatter', x='col1', y='col2', mode='markers')\n",
    "df.iplot(kind='bar', x='cat_col', y='col')\n",
    "df.sum().iplot(kind='bar'....) # same as above but you add sum() or count() for non-categorical data\n",
    "df.iplot(kind='surface', colorscale='rdylbu') # 3D grpah\n",
    "df['col'].iplot(kind='hist', bins=20) # you can also pass a whole df\n",
    "df.iplot(kind='spread')\n",
    "df.iplot(kind='bubble', x='col1', y='col2', size='col3')\n",
    "df.scatter_matrix() # needs all columns to be numerical. SLOW when lots of data\n",
    "\n",
    "# Geographical Plotting\n",
    "# Example:\n",
    "''' Need to setup like for Plotly and Cufflinks. Plus:'''\n",
    "import plotly.graph_objs as go\n",
    "# US MAPS\n",
    "my_data = dict(type = 'choropleth', \n",
    "               locations=df['state codes'], \n",
    "               locationmode='USA-states',\n",
    "               colorscale='YlOrRd', \n",
    "               text=df['text to read'],\n",
    "               marker= dict(line=dict(color='rgb(255,255,255)', width=2)), \n",
    "               z='list with values for color state', colorbar={'title': 'Some Title'})\n",
    "my_layout = dict(geo={'scope':'usa'})\n",
    "choromap = go.Figure(data=[my_data], layout=my_layout)\n",
    "iplot(choromap) # remove the 'i' to open in new page and save plot\n",
    "# WORLD MAPS - Example\n",
    "my_data = dict(type='choropleth', \n",
    "               locations='list of country codes', \n",
    "               z=df['col'],\n",
    "               text=df['col'], \n",
    "               locatoinmode='country names',\n",
    "               reversescale=True,\n",
    "               colorbar={'title':'TITLE'})\n",
    "layout=dict(title='title', geo=dict(showframe=False, projection={'type':'Mercator'}))\n",
    "choromap2 = go.Figure(data=[my_data], layout=layout)\n",
    "iplot(choromap2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: The Iris Setosa\n",
    "from IPython.display import Image\n",
    "url = 'http://upload.wikimedia.org/wikipedia/commons/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg'\n",
    "Image(url,width=300, height=30)\n",
    "\n",
    "# Load and Plot using Numpy\n",
    "from IPython.display import Image\n",
    "from PIL import Image\n",
    "img = Image.open(\"bird_small.png\")\n",
    "data = np.asarray(img)\n",
    "plt.imshow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3 Printing and Displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text file named as follows, with ~ being the file path before ipython\n",
    "# ~/.ipython/profile_default/ipython_config.py\n",
    "\n",
    "# Write the following in the text file:\n",
    "\n",
    "c = get_config()\n",
    "# Run all nodes interactively\n",
    "c.InteractiveShell.ast_node_interactivity = \"all\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
