{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning & Data Analytics Cheat Sheet\n",
    "\n",
    "This notebook contains useful Python code for Machine Learning practices.\n",
    "- by: Daniel Bejarano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Saved Datasets\n",
    "from sklearan.datasets import load_boston\n",
    "boston = load_boston()\n",
    "boston['data'] # to access X\n",
    "boston['feature_names'] \n",
    "boston['target'] # to access y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "\n",
    "# Fill Null Values Based on Classes Mean Values\n",
    "def impute_age(cols):\n",
    "    age = cols[0]\n",
    "    pclass = cols[1]\n",
    "    if pd.isnull(age):\n",
    "        if pclass==1:\n",
    "            return 'mean for pclass 1'\n",
    "        elif pclass==2:\n",
    "            return 'mean for pclass 2'\n",
    "        else:\n",
    "            return 'mean for pclass 3'\n",
    "    else:\n",
    "        return age\n",
    "df['age'] = df[['age', 'pclass']].apply(impute_age, axis=1)\n",
    "\n",
    "# Imputer\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'string shown on missing values',\n",
    "                 strategy='mean', axis=0) #axis=0 takes mean of the columns\n",
    "imputer.fit(X['columns_with_missing_data'])\n",
    "X['columns_with_missing_data'] = imputer.transform(X['columns_with_missing_data'])\n",
    "\n",
    "# Droping\n",
    "df.drop('col', axis=1, inplace=True) # an entire column\n",
    "df.dropna(inplace=True) # rows with missing values\n",
    "\n",
    "# Fill NA values\n",
    "df['A'].fillna(value=df['A'].mean())\n",
    "\n",
    "# Remove Duplicates\n",
    "df.duplicated()\n",
    "\n",
    "# Normalization and Standardization\n",
    "x -= x.mean(axis=0)\n",
    "x /= x.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pre-Processing Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dummy Variables\n",
    "cat_feats = ['col1', 'col2']\n",
    "final_data = pd.get_dummies(df, columns=cat_feats, drop_first=True) # removes one of them to avoid redundant information\n",
    "#OR\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X =onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "# Creating labelencoders - UNLESS THE CATEGORIES HAVE A RELATINOAL ORDER, USE DUMMY VARIABLES\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X['cat_column'] = labelEncoder.fit_transform(X['cat_column'])\n",
    "\n",
    "# Write Cleansed Data to file\n",
    "df.to_csv('cleaned_df', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train-Val-Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Split before performing EDA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "train_data, test_data = train_test_split(df, test_size=0.25) # when don't want to define X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Normalization & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # scale can greatly affect the result of the model\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df.drop('y', axis=1))\n",
    "scaled_X = scaler.transform(df.drop('y', axis=1))\n",
    "X = pd.DataFrame(scaled_X, columns=df.columns[:-1]) # assuming y is the last column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis') \n",
    "    # great visualization of missing (NA, NaN, null) values\n",
    "    \n",
    "# Basic Data Exploration\n",
    "df.info()\n",
    "sns.pairplot(df)\n",
    "sns.distplot(df.y)\n",
    "sns.heatmap(df.corr(annot=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine Tuning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Preferable to be done with GridSearchCV'''\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "parameters = [{'C': [1,10], 'kernel':['linear']}\n",
    "             {'C':[1,10], 'kernel':['rbf'], 'gamma':[0.5, 0.1]}]\n",
    "grid_s = GridSearchCV(SVC(), param_grid=parameters, scoring='accuracy', cv=10, refit=True, verbose=3)\n",
    "grid_s = grid_s.fit(X_train, y_train)\n",
    "best_accuracy = grid_s.best_score_\n",
    "best_params = grid_s.best_params_\n",
    "\n",
    "RandomizedSearchCV # alternative to GridSearchCV where you can explore more parameter options and \n",
    "# control the number of iterations - good for when needing to explore many many hyperparameters/models\n",
    "\n",
    "# A more flexible, more manual approach that stratisfies the splits:\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train):\n",
    "    clone_model = clone(model)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train[train_index]\n",
    "    X_test-fold = X_test[test_index]\n",
    "    y_test_fold = y_test[test_index]\n",
    "    \n",
    "    clone_model.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_model.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold) # for classification\n",
    "    print(n_correct / len(y_pred))\n",
    "    \n",
    "# Doing Cross Val without GridSearch\n",
    "accuracies = cross_val_score(estimator=name_of_model, X=X_train, y=y_train, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID-SEARCH + PIPELINES - Kernel PCA Example as preprocessing and Logistic \n",
    "\n",
    "clf = Pipeline([\n",
    "        ('kpca', KernelPCA(n_components=2)),\n",
    "        ('log_reg', LogisticRegression())\n",
    "        ])\n",
    "\n",
    "param_grid = [{\n",
    "        'kpca_gamma': np.linspace(0.03, 0.05, 10),\n",
    "        'kpca_kernel': ['rbf', 'sigmoid']\n",
    "        }]\n",
    "\n",
    "grid_search = GrisSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X,y)\n",
    "grid_Search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "log_clf = LogisticRegression()\n",
    "svm_clf = SVC()\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('svc', svc_clf)],\n",
    "                              voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "'''where the estimators are a list of classifiers. The more and more diverse \n",
    "they are the better'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Bagging\n",
    "Bagging as shown below is analogous to using RandomForests using a Decision Tree classifier. Meaning, if we used BaggingClassifier with a DecisionsTreeClassifier is the same as if we used RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(svm_clf, n_estimators=500, max_samples=100,\n",
    "                           bootstrap=True, n_jobs=1,oob_score=True)\n",
    "# fit and test as usual\n",
    "bag.clf.oob_score_ #gives the automatically calculated out-of-bag accuracy rate\n",
    "# which is a good approximation of the error to be expected on the test set\n",
    "bag_clf.oob_decision_function_ # class's probability estimate, if model computes it\n",
    "'''n_jobs is the number of CPU cores to use (-1 means all available cores)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 ADA Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                             n_estimators=400, algorithm='SAMME.R', learning_rate=1)\n",
    "# SAMME.R is a multiclass ADA booster algorithm which uses class probability estimates\n",
    "# SAMME uses predictions instead of probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Gradient Boosting\n",
    "Can use different cost functions, if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=.5)\n",
    "\n",
    "# Findind the optimal number of predictors\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in\n",
    "         gbrt.staged_predict(X_val)] \n",
    "'''staged_predict() returns an iterator over the predictions made by\n",
    "the ensemble at each stage of training (with 1 tree, 2 trees, 3...),\n",
    "from where we can get the number of estimators that (1, 2, 3...) with min error'''\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 XGBoost\n",
    "To be used with neural networks mainly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. High performance\n",
    "# 2. Fast execution speed\n",
    "# 3. Get to keep interpretation of model\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Stacking\n",
    "Not supported by scikit-learn...yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Machine Learning Models and Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0 Evauation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.0.1 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test, pred)) # OR\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recalls, thresholds = precision_recall_curve(y_train, y_pred)\n",
    "\n",
    "# Plot Prediction Boundaries \n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plot_decision_regions(X, y, clf=model_name, legend=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train, y_train)\n",
    "pred = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.1 Simple and Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train) # you can grab lm.intercept_, lm.coef_\n",
    "cdf = pd.DataFrame(np.transpose(lm.coeff_), X.columns, columns = ['Coeff'])\n",
    "\n",
    "# Alternative: Fit model using stat API - Provides a more comprehensive set of model parameters\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(lstat_df)\n",
    "est = sm.GLS(y, X)\n",
    "est2 = est.fit()\n",
    "pred = est2.predict()\n",
    "print(est2.summary())\n",
    "\n",
    "# Predict\n",
    "pred = lm.predict(X_test)\n",
    "plt.scatter(y_test, pred)\n",
    "sns.distplot((y_test-pred)) # histogram of the residuals - Normally distributed residuals is a good sign\n",
    "\n",
    "# Metrics to evaluate model\n",
    "metrics.mean_absolute_error(y_test, pred) # easy to interpret: average error\n",
    "metrics.mean_squared_error(y_test, pred) # preferable because it punishes larger errors more greatly\n",
    "np.sqrt(metrics.mean_squared_error(y_test, pred)) # Root Mean Squared Error - Same units as y\n",
    "\n",
    "# Polynomial Regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_fet = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_feat.fit_transform(X) # now contains X and X^2 features\n",
    "'''Note that if more than 1 columns are given, it will return all the combinations\n",
    "of the features that will give the degree specified. If df = x1, x2, x3 and\n",
    "degree=2, we will get 1, x1, x2, x3, x1^2, x1*x2, x1*x3, x2^2, x2^x3, x3^2'''\n",
    "lin_reg.fit_(X_poly, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2.2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "# A) Using the Closed Form (Normal) Equation\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver='cholesky')\n",
    "'''fit and predict as usual'''\n",
    "# B) Using Stochastic Gradient Descent\n",
    "# Great Source: https://iamtrask.github.io/2015/07/27/python-network-part2/\n",
    "sgd_reg = SGDRegressor(penalty='l2')\n",
    "''' fit and predict as usual'''\n",
    "\n",
    "# Lasso Regression\n",
    "sgd_reg = SGDRegressor(penalty='l1') # OR...\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1) '''fit and predict as usual'''\n",
    "\n",
    "# Elastic Net\n",
    "from sklearn.linear_model import Elasticnet\n",
    "el_net = EleasticNet(alpha=0.1, l1_ratio=0.5) # half Ridge, half Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Naive Bayes\n",
    "It's used mainly in Linear Discriminant Analysis but is itself a classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GAussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "pred = classifier.predict(X_test)\n",
    "\n",
    "# Example: Naive Bayes to classify emails\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_model = MultinomialNB().fit(mess_tfidf, df['label'])\n",
    "pred = spam_model.predict(mess_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Remember to normalize and scale values prior to fitting model. See Data Pre-Processing section above'''\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KneighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Determine K iteratively \n",
    "'''This can be substituted with GridSearchCV'''\n",
    "error_rate = []\n",
    "for i in range(1:30):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Tree-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.1 Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, y_train)\n",
    "pred = dtree.predict(X_test)\n",
    "\n",
    "# Regression\n",
    "from sklearn.tree import DecisionTreeRegressor #fit and train as usual\n",
    "\n",
    "# Visualization of Trees\n",
    "'''Not useful for Random Forests, but useful for single trees'''\n",
    "from IPython.display import Image\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "\n",
    "features = list(df.columns[1:])\n",
    "dot_data = StringIO()\n",
    "export_graphviz(dtree, out_file=dot_data, feature_names=features, filled=True)\n",
    "                \n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "Image(graph[0].create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.2 Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfc = RandomForestClassifier(n=estimators = 200)\n",
    "#...\n",
    "rfc.feature_importances_\n",
    "\n",
    "# Extremely Randomized Trees\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Perform Feature Scaling prior to fitting model'''\n",
    "'''In SVMs, must set \"probability=True\" for it to provide class probabilities'''\n",
    "\n",
    "from sklearn.svm import SVC \n",
    "from sklern.svm import LinearSVC # much faster than SVC, assuming linear\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Specifying Parameters \n",
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(X_train, y_train)\n",
    "pred = svc.preict(X_test) \n",
    "# classification_report and confusion_matrix as usual\n",
    "\n",
    "# Finding the Best Parameters - ALWAYS USE THIS FOR SVM\n",
    "'''Since grid search can take very long, one suggestion is to perform your grid search on a small \n",
    "copy of your dataset or using a small subset of the parameter values you want to try, to make sure \n",
    "it works as expected. Then do it on whole dataset and using all the desired parameters over night \n",
    "or something'''\n",
    "param_grid = {'C':[.1, 1, 10], 'gamma':[1, .1, .01]}\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3) # set verbose to at least 2 to know if model is doing something\n",
    "grid.best_params_\n",
    "grid.best_estimator_\n",
    "grid_pred = grid.predict(X_test)\n",
    "# classification_report and confusion_matrix as usual\n",
    "\n",
    "'''Remember to scale back predictions when doing SVR (regression).'''\n",
    "y = StandardScaler()\n",
    "ss_y = y.fit(y)\n",
    "y_pred = ss_y.inverse_transform(predictions)\n",
    "\n",
    "# For huge datasets, use SGDClassifier, since it can do out-of-core training\n",
    "sgd_svm = SGDClassifier(loss='hinge', alpha=1/(m*C)) # data must be scaled and centered around 0\n",
    "'''applies SGD to train a linear SVM classifier'''\n",
    "\n",
    "# SVM Regression #\n",
    "from sklearn.svm import LinearSVR\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "# epsilon acts as C in classification - determines the width of the margin\n",
    "\n",
    "# For Multivariate Classification\n",
    "'''Random Forests and Bayes can do it directly, but SGD and SVM can't'''\n",
    "from sklearn.multiclass import OneVsOneClassifier # OR OneVsAllClassifier\n",
    "model = OneVsOneClassifier(SGDClassifier()) # or SVM or other \n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Apriori\n",
    "Simple basic way of determining likelihood that a certain action happens given another action. Recommender Systems is a more robust version of it. How likely are customers to buy bananas if they bought chips? Steps:\n",
    "1. Set a minimum support and confidence\n",
    "2. Take all the subsets in transactions having higher support than minimum support\n",
    "3. Take all the rules of these subsets having higher confidence than the minimum confidence\n",
    "4. Sort hte rules by decreasing lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "rules = apriori(transactions, min_support=0.003, min_confidence=0.2, min_lift=3, min_length=2)\n",
    "\n",
    "# Eclat\n",
    "# Step 1: Set a minimum support\n",
    "# Step 2: take all the subsets in transactions having higher support than minimum support\n",
    "# Step 3: Sort these subsets by decreasing support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 K-Means Clustering\n",
    "The goal is not to predict something, but rather to find patterns in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.8.1 Basic K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(points)\n",
    "kmeans.cluster_centers_\n",
    "kmeans.labels_\n",
    "\n",
    "# Pringint Graph with labels\n",
    "fig, ax = plt.figure(figsize=(10, 6))\n",
    "ax.scatter(df['col1'], data['col2'], c=kmeans.labels_, cmap='rainbow')\n",
    "\n",
    "# Using the Elbow Method to Choose K  (which also uses k-means++) !\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    plt.plot(range(1,11), wcss)\n",
    "'''From observing the graph choose k based on the elbow. Here, assume 5 is the best'''\n",
    "kmeans = KMeans(n_cluster=5, init='k-means++', max_iter=300, n_init=10)\n",
    "y_kmeans = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.8.2 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "dendogram = sch.dendogram(sch.linkage(X, method='waard'))\n",
    "plt.plot(dendogram)\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\n",
    "y_hc hc.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.9 Dimensionality Reduction / Feature Extraction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.9.1 Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA without Scikit-Learn (for funsies)\n",
    "X_centered = X-X.mean(axis=0)\n",
    "U, s, Vt= np.linalg.svd(X_centered)\n",
    "C1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]\n",
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)\n",
    "\n",
    "# PCA w/ Scikit-Learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "scaled_data = scaler.transform(X)\n",
    "\n",
    "pca = PCA(n_components=2) # OR you can specify a number between 0 and 1...\n",
    "pca = PCA(n_components=0.95) # which gives you enough components to explain 95% of the variance\n",
    "pca.fit(scaled_data)\n",
    "x_pca = pca.transform(scaled_data)\n",
    "x_pca.shape\n",
    "first_component = x_pca.components_.T[:, 0]\n",
    "evr = x_pca.explained_variance_ratio_ # how much of the variance lies on each PC\n",
    "\n",
    "# Visualization of Components\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x_pca[:,0], x_pca[:,1], c=df['target column']) # added color to see if target variable is discernable in the PCs\n",
    "pca.components_\n",
    "df_comp = pd.DataFrame(pca.components_, columns=df.columns)\n",
    "sns.heatmap(df_comp, cmap='plasma') # the hotter the color the more correlated to a feature the PCs are\n",
    "\n",
    "# To decompress data back to its original form - although some info has been lost\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.9.2 Incremental PCA\n",
    "For VERY large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pa = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pa.partial_fit(X_batch)\n",
    "    \n",
    "X_reduced= inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.9.3 Randomized PCA\n",
    "Faster when d is way smaller than n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver='randomized')\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.9.4 Kernel PCA\n",
    "Complex nonlinear projections for dimensionality reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "rbc_pca = KernelPCA(n_componets=2, kernel='rbf', gamma=0.04)\n",
    "X_train_reduced = rbc_pca.fit_transform(X_train)\n",
    "X_test_reduced = rbc_pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.9.5 Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.disccriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "X_train = LDA.fit_transform(X_train, y_train)\n",
    "X_test = LDA.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.10 Recommender Systems\n",
    "\n",
    "STEPS for Movie Reviews Example:\n",
    "1. Use pd.pivot_table to create a matrix. Rows as users. Columns as movies reviewed. Values as ratings \n",
    "2. similar_to_starwars = matrix_df.corrwith(starwars_ratings)\n",
    "3. Filter out movies with less than y number of reviews - to avoid high correlation between movies that just  ONE person reviewed, who also happened to review StarWars\n",
    "\n",
    "For a BETTER example look at assignment from Coursera's ML Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.11 Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.11.1 Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "nltk.download_shell() # do \"d\" and then \"stopwords\" to download the stopwords packages\n",
    "\n",
    "def text_process(mess):# mess stands for message\n",
    "    '''\n",
    "    1. Remove punctuation\n",
    "    2. Remove Stop Words\n",
    "    3. Return list of cleaned text words\n",
    "    '''\n",
    "    ps = PorterStemmer()\n",
    "    nopunc = [c for c in mess if c not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc) # to join together separated characters back into words\n",
    "    return [ps.stem(w) for w in nopunc.split() if w.lower() not in stopwords.words('english')]\n",
    "\n",
    "''' Stemming and lemmitization perform badly when we have short-hand words like u for you, etc\n",
    "BUT, for other purposes we NEED TO ADD STEMMING AND LEMMITIZATION TO FUNCTION ABOVE !!! '''\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "result = [lemmatizer.lemmatize(t) for t in items]\n",
    "\n",
    "bow_transformer = CountVectorizer(analyzer=text_process, max_features=2000).fit(df['mess'])\n",
    "# bag of words. We add our own analyzer to avoid specifying the large number of parameters CountVectorizer requires\n",
    "bow_transformer.vocabulary_ # gives the number of words in our vocabulary\n",
    "bow_transformer.get_feature_names()[a_number] # gives the word at index a_number\n",
    "mess_bow = bow_transformer.transform(df['mess'])\n",
    "\n",
    "# Check the resulting BoW for all messages \n",
    "sparcity = (100.0 * mess_bow.nnz / (mess_bow.shape[0] * mess_bow.shape[1])) # nonzeros/total\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_transformer = TfidfTransformer().fit(mess_bow)\n",
    "mess_tfidf = tfidf.transform(mess_bow)\n",
    "tfidf_transformer.idf_[bow_transformer.vocabulary_['universe']] # checks the TF-IDF score for 'universe'\n",
    "\n",
    "# Using a Pipeline \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('bow', CountVetorizer(analyzer=text_process)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "classification_report(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.11.2 Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) CREATE YOUR OWN ONE-HOT ENCODING\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(my_text)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(my_text)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(my_text, mode='binary')\n",
    "\n",
    "word_index = tokenixer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# B) CREATE YOUR OWN WORD EMBEDDINGS\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100\n",
    "training_samples = 5000\n",
    "validation_samples = 5000\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words) \n",
    "tokenizer.fit_on_texts(texts) # creates a list of tokens (words) of the 10000 most common words on \"texts\"\n",
    "sequences = tokenizer.texts_to_sequences(texts) # converts \"texts\" into sequences of tokens for the 10000 most common words\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "X = pad_sequences(sequences, maxlen=maxlen)\n",
    "y = np.asarray(labels)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(10000, 64) \n",
    "    # where 10000 is = 1 + max word index (in other words, is the number of words)\n",
    "    # and 64 is the dimensionality of the embeddings (size of the vectors)\n",
    "    # The input to the layer must be batches of observations of the SAME SIZE. If the size is 100, then all inputs (say \n",
    "    # Amazon reviews for instance) longer than 100 words will be cut, and all those shorter than 100 need to be padded\n",
    "\n",
    "# C) USING PRE-TRAINED EMBEDDINGS\n",
    "glove_dir = '\\\\Users\\\\dbejarano\\\\Dropbox\\\\Jupyter Notebooks\\\\Pre-Trained Basis'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "# Prepare the GloVe word-embeddings matrix\n",
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items(): # runs through the word index\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen)) \n",
    "    # this layers takes observations as inputs,\n",
    "    # which in this case they are of size 100 (100 words, represented as an index number which, through the word_index\n",
    "    # dictionary gives access to the actual word). The layer matches that number to the weights at that position. Meaning\n",
    "    # if the first number in observation i is 4, then it will output the vector for word number 4. In conclusion,\n",
    "    # it changes the word index number for each word in the observation with the embedding vector from GloVe for that word\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# Load the Pretrained GloVe embedding into the network\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "# Train the Network\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=11, batch_size=32, validation_data=(X_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.13 Dense Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources\n",
    "\n",
    "- Walk-through example of a neural network, with actual numbers:\n",
    "    - https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "- Keras: \n",
    "    - https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.13.1 With Sequential Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(input_dim=10, output_dim=6, init='uniform', activation='relu')) # first hidden layer\n",
    "classifier.add(Dense(input_dim=6, output_dim=6, init='uniform', activation='relu')) # second hidden layer\n",
    "classifier.add(Dense(input_dim=6, output_dim=1, init='uniform', activation='sigmoid')) # output layer\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy']) \n",
    "classifier.fit(X_train, y_train, batch_size=10, nb_epoch=100, epochs=4, validation_data=(X_val, y_val))\n",
    "final_results = classifier.evaluate(X_test, y_test) # contains the error and metric value\n",
    "pred = classifier.predict(X_test)\n",
    "\n",
    "# One-Hot Encoding of Categorical Variables\n",
    "from keras.utils.np_utils import to_categorical\n",
    "one_hot_y_train = to_categorical(y_train)\n",
    "\n",
    "# REGULARIZATION #\n",
    "# 1. Get more Training Data\n",
    "\n",
    "# 2. Reduce the Flexibility/Capacity/Complexity of the Model\n",
    "\n",
    "# 3. l1 AND l2\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu')) # for L2, but can change to \"l1\"\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001), activation='relu'))\n",
    "\n",
    "# 4. Dropout\n",
    "model.add(Dense(input_dim=10, output_dim=6, init='uniform', activation='relu')) \n",
    "model.add(layers.Dropout(0.5)) \n",
    "    # Dropout scales up the active neurons at training, which is the same as scaling \n",
    "    # down the larger number of active neurons at testing\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# ... Repeat for other layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.13.2 With Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "\n",
    "input_tensor = Input(shape=(64,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(input_tensor, output_tensor)\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128)\n",
    "score = model.evaluate(X_train, y_train)\n",
    "\n",
    "# MULTI-INPUT MODELS #\n",
    "input_1 = Input(shape=(None, ), dtype='int32', name='text')\n",
    "embedded_input_1 = layers.Embedding(64, text_vocab_size)(input_1)\n",
    "encoded_input_1 = layers.LSTM(16)(embedded_input_1)\n",
    "\n",
    "input_2 = Input(shape=(None, ), dtype='int32', name='question')\n",
    "embedded_input_2 = layers.Embedding(32, quest_vocab_size)(input_2)\n",
    "encoded_input_2 = layers.LSTM(16)(embedded_input_2)\n",
    "\n",
    "concatenated = layers.concatenate([encoded_input_1, encoded_input_2], axis=-1)\n",
    "\n",
    "output = layers.Dense(answer_vocab_size, activation='softmax')(concatenated) # returns probabilities for 500 words\n",
    "\n",
    "model = Model([input_1, input_2], output)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.fit([X1_train, X2_train], y_train, epochs=10, batch_size=128)\n",
    "\n",
    "# MULTI OUTPUT MODELS #\n",
    "input_tensor = Input(shape=(None, ), dtype='int32', name='posts')\n",
    "embedded_tensor = layers.Embedding(256, vocabulary_size)(input_tensor)\n",
    "\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_tensor)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "y1_pred = layers.Dense(1, name='age')(x) # this example prdicts age, income and gender of a user\n",
    "y2_pred = layers.Dense(income_groups, activation='softmax', name='income')(x)\n",
    "y3_pred = layers.Dense(1, activation='sigmoid', name='gender')(x)\n",
    "\n",
    "model = Model(input_tensor, [y1_pred, y2_pred, y3_pred])\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'], \n",
    "             loss_weights=[.25, 1., 10.])\n",
    "\n",
    "model.fit(X_train, [y1_train, y2_train, y3_train])\n",
    "\n",
    "# ACYCLIC GRAPHS OF LAYERS #\n",
    "branch_a = layers.Conv2D(128, 1,activation='relu', strides=2)(x)\n",
    "branch_b = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_b = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_b)\n",
    "\n",
    "branch_c = layers.AveragePooling2D(3, strides=2)(x)\n",
    "branch_c = layers.Conv2D(128, 3, activation='relu')(branch_c)\n",
    "\n",
    "branch_d = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu')(branch_d)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_d)\n",
    "output = layers.concatenate([branch_a, branch_b, branch_c, branch_d], axis=-1)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continued from cell above\n",
    "\n",
    "# RESIDUAL CONNECTIONS #\n",
    "# Case when the feature-map sizes are the same, using identity residual connections\n",
    "x = ... # where x is a 4D input tensor\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.add([y, x])\n",
    "\n",
    "# Case when the feature-map sizes differ, using alinear residual connection\n",
    "x = ... # where x is a 4D input tensor\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.MaxPooling2D(2, strides=2)(y)\n",
    "\n",
    "residual = layers.Conv2D(128, 1, strides=2, padding='same')(x)\n",
    "y = layers.add([y, residual])\n",
    "\n",
    "# CALLBACKS #\n",
    "# Some of the available callbacks\n",
    "keras.callbacks.ModelCheckpoint\n",
    "keras.callbacks.EarlyStopping\n",
    "keras.callbacks.LearningRateScheduler\n",
    "keras.callbacks.ReduceLROnPlateau\n",
    "keras.callbacks.CSVLogger\n",
    "\n",
    "# Early stopping and model checkpoint callbacks\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='acc',patience=1,),\n",
    "                  keras.callbacks.ModelCheckpoint(filepath='my_model.h5',monitor='val_loss',save_best_only=True,)]\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(x, y, epochs=10, batch_size=32, callbacks=callbacks_list, validation_data=(x_val, y_val))\n",
    "\n",
    "# Reducing rate of learning when loss reduction plateaus\n",
    "callbacks_list = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)]\n",
    "\n",
    "model.fit(x, y, epochs=10, batch_size=32, callbacks=callbacks_list, validation_data=(x_val, y_val))\n",
    "\n",
    "# TENSORBOARD #\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=folder_dir, histogram_freq=1, embeddings_freq=1)]\n",
    "history = model.fit(x_train, y_train, epochs=20,batch_size=128, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "# Take a look at graph visualizations\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True, to_file='example_model.png') # shows the topology of the network\n",
    "\n",
    "# BATCH NORMALIZATION #\n",
    "# After a Conv layer\n",
    "conv_model.add(layers.Conv2D(32, 3, activation='relu'))\n",
    "conv_model.add(layers.BatchNormalization())\n",
    "# After a dense layer\n",
    "dense_model.add(layers.Dense(32, activation='relu'))\n",
    "dense_model.add(layers.BatchNormalization())\n",
    "\n",
    "# DEPTHWISE SEPARABLE CONVOLUTION #\n",
    "model.add(layers.SeparableConv2D(128, 3, activation='relu'))\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "# MODEL ENSEMBLING #\n",
    "# Including model weights learned during validation\n",
    "# To search for a good set of ensembling weighs use an optimization algorithm such as Nelder-Mead\n",
    "preds_a = model_a.predict(x_val)\n",
    "preds_b = model_b.predict(x_val)\n",
    "preds_c = model_c.predict(x_val)\n",
    "final_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.13.3 Using TensorFlow Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "str_const = tf.constant('a_string') # creating a tensor object of a constant\n",
    "num_const = tf.constant(100) # same as above but with a number\n",
    "\n",
    "# Session Object - Encapsulates the environment in which operation objects are evaluated\n",
    "sess = tf.Session()\n",
    "sess.run(str_const) # Running the constant (just an example)\n",
    "type(sess.run(str_const)) # gives back the type\n",
    "\n",
    "# Operations - Multiple can be lined to run during a session\n",
    "mat1 = np.array([[5.0, 5.0]])\n",
    "mat2 = np.array([[2.0], [2.0]])\n",
    "x = tf.constant(mat1)\n",
    "y = tf.constant(mat2)\n",
    "with tf.Session() as sess:\n",
    "    print('Multiplicaton', sess.run(x*y)) # OR...\n",
    "mat_multi = tf.matmul(x, y)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(mat_multi))\n",
    "    \n",
    "# Operations with Placeholders    \n",
    "x = tf.placeholder(tf.int32) # Creates a placeholder for a constant by specifying the type\n",
    "y = tf.placeholder(tf.int32) # might need to change 32 to 64 depending on the computer bits\n",
    "add = tf.add(x,y) # Analogous to functions in python because we haven't specified value of x and y\n",
    "mul = tf.mul(x,y) \n",
    "with tf.Session() as sess:\n",
    "    print('Addition', sess.run(add, feed_dict={x:20, y:30}))\n",
    "    print('Multiplicaton', sess.run(mul, feed_dict={x:20, y:40}))\n",
    "    \n",
    "# APPROACH 1: MANUAL GRAPH DEFINITION - MNIST EXAMPLE \n",
    "x = tf.placeholder(tf.float32, shape=[None, 784]) # where 'None' is a placeholder for the batch size, as giving all images at once is too much for the model\n",
    "W = tf.Variable(tf.zeros([784,10])) # weights\n",
    "b = tf.Variables(tf.zeros([10])) # biases - need them so we don't get cases where an input to a neuron is 0\n",
    "y = tf.matmul(x, W) + b # this is the graph\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10]) # where we will add the correct labels\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=y)) # loss function - defines the error\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5) # Optimizer - how we minimize the error. Faster rates tend to be less accurate\n",
    "# Train\n",
    "train = optimizer.minimize(cross_entropy)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(1000):\n",
    "        batch_x, batch_y = df.next_batch(100)\n",
    "        sess.run(train, feed_dict={x:batch_x, y:batch_y})\n",
    "    matches = tf.equal(tf.argmax(y,1), tr.argmax(y_true, 1)) # when the model predicted right\n",
    "    acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "    print(sess.fun(acc, feed_dict={x:test_df.images, y_true:test_df.labels}))\n",
    "    \n",
    "# APPROACH 2: USING TENSORFLOW ESTIMATOR OBJECTS - MNIST EXAMPLE\n",
    "'''Notes\n",
    "1. Column names can't have spaces or special characters\n",
    "2. The target column values must be integers\n",
    "3. Scale the Data\n",
    "'''\n",
    "# 1. Create feature columns\n",
    "feat_cols = []\n",
    "for col in X.columns:\n",
    "    feat_cols.append(tf.feature_columns.numeric_column(col))\n",
    "# 2. Create Input Function\n",
    "input_func = tf.estimator.inputs.pandas_input_fn(x=X_train, y=y_train, batch_size=10, num_epochs=5, shuffle=True) #epoch is going through all the training data at least once\n",
    "# 3. Create Estimator - #each num in units list is the number of neurons at each layer\n",
    "classifier = tf.estimator.DNNClassifier(hidden_units=[10, 20, 10], n_classes=3,feature_columns=feat_cols) \n",
    "# 4. Train Estimator\n",
    "classifier.train(input_fn=input_func, steps=50)\n",
    "# 5. Test Estimator\n",
    "pred_fn = tf.estimator.inputs.pandas_input_fn(x=X_test, batch_size=len(X_test), shuffle=False)\n",
    "pred = list(classifier.predict(input_fn=pred_fn))\n",
    "final_preds = []\n",
    "for p in pred:\n",
    "    final_preds.append(p['class_ids'][0])\n",
    "classification_report(y_test, final_preds)\n",
    "\n",
    "# APPROACH 3: CREATE VIA FUNCTION AND LOOP #\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None,\n",
    "        activation=tf.nn.elu, initializer=he_init):\n",
    "    with tf.variable_scope(name, \"dnn\"):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation=activation,\n",
    "                                     kernel_initializer=initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "        return inputs\n",
    "\n",
    "# He Initialization - To randonmly initialize the weights #\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, \n",
    "                         kernel_initializer=he_init, name='hidden1')\n",
    "\n",
    "# ELU Activation Function - A step up from ReLU #\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name='hidden1')\n",
    "\n",
    "# BATCH NORMALIZATION\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden, name='hidden1')\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn1_act, n_outputs, name='outputs')\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                      momentum=0.9)\n",
    "\n",
    "# BATCH NORMALIZATION USING CLEANER CODE # \n",
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training,\n",
    "                             momentum=0.9)\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1')\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "logits_before_bn = tf.layers.dense(bn1_act, n_outputs, name='outputs')\n",
    "logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "# Execution Phase\n",
    "extra_update_ops = tf.get_collections(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteratino in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                    feed_dict={training: True, X: X_batch, y:y_batch})\n",
    "        accuracy_val = accuracy_eval(feed_dict={X:mnist.test.images,\n",
    "                                               y:mnist.test.labels})\n",
    "        print(epoch, 'Test accuracy:', accuracy_val)\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "\n",
    "# GRADIENT CLIPPING\n",
    "threshold = 1.0\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_balue(grad, -threshold, threshold), var)\n",
    "             for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradient(capped_gvs)\n",
    "\n",
    "# REUSING PRETRAINED LAYERS #\n",
    "import_meta_graph() # imports operations from the pretrained layers into default graph\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "get_operation_by_name() # returns the operationsyou'll need for training\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")\n",
    "\n",
    "'''The above is assuming the pretrained model is well documented. Otherwise,\n",
    "you can use the following to get the operations'''\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)\n",
    "    \n",
    "# Freezing the lower layers\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                              scope='hidden[34]|outputs')\n",
    "training_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "'''The above gives the model only those layers to train: 3, 4 and output'''\n",
    "\n",
    "# FASTER OPTIMIZERS #\n",
    "tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)\n",
    "# Nesterov is preferred over vanilla momentum\n",
    "tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.9,\n",
    "                         decay=0.9, epsilon=1e-10)\n",
    "tf.train,AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# LEARNING RATE SCHEDULE\n",
    "initial_learning_rate = 0.1\n",
    "decay_steps = 10000\n",
    "decay_rate = 1/10\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                          decay_steps, decay_rate)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "# REGULARIZATION #\n",
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu, \n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None, name='outputs')\n",
    "    \n",
    "\n",
    "# DROPOUT #\n",
    "dropout_rate = 0.5 # == 1-keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training_training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name='hidden1')\n",
    "    hidden1_drop = tf.layers.dropout(hidden, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden1_drop, n_outputs, name='outputs')\n",
    "    \n",
    "# MAX-NORM REGULARIZATION #\n",
    "def max_norm_regularizer(threshold, axes=1, name='max_norm',\n",
    "                        collection='max_norm')\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regulatization loss term\n",
    "    \n",
    "    return max_norm\n",
    "\n",
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_Scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden, activation=tf.nn.rely,\n",
    "                             kernel_regularizer=max_norm_reg, name='hidden1')\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")\n",
    "    \n",
    "clip_all_Weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "        acc_valid = accuracy_eval(feed_dict={X: x_valid, y: y_valid})\n",
    "        print(epoch, \"validation accuracy:\", acc_valid)\n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.14 Convoluted Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.14.1 Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolutional\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Convolution2D(32, 3, 3, input_shape=(64, 64, 3), activatoin='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2))) #reduces complexity w/o reducing performance\n",
    "classifier.add(Convolution2D(32, 3, 3, activation='relu')) # 2nd Conv Layer\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2))) # 2nd Conv Layer\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(output_dim=128, activation='relu')) #hidden layer \n",
    "classifier.add(Dense(output_dim=1, activation='sigmoid')) #output layer\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# IMAGE AUGMENTATION\n",
    "# Used to reduce overfitting\n",
    "\n",
    "# 1. ImageDataGenerator - performs random transformations to image (flip, rotate, etc)\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, \n",
    "                                   zoom_range=0.2, horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset_train_set', \n",
    "                                                target_size=(64,64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                                        target_size=(64, 64),\n",
    "                                                        batch_size=32,\n",
    "                                                         class_mode='binary')\n",
    "\n",
    "model.fit_generator(train_set, steps_per_epoch=1000, epochs=25,\n",
    "                    validation_data=test_set, validation_steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.14.1 Using TensorFlow\n",
    "\n",
    "Sources:\n",
    "- GREAT visual representation:\n",
    "    - scs.ryerson.ca/~aharley/vis/conv/flat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
    "filters = [:3, :, 0] = 1 # creates a vertical line\n",
    "filters[3, :, :, 1] = 1 # creates a horizontal line\n",
    "'''In this example we are creating the filters. In a real CNN you let the algorithm find them'''\n",
    "\n",
    "# Create graph with input X plus convolutional layer applying the filters\n",
    "X = tf.placeholder(tf.float32, shape=(None, height, width, channels))\n",
    "convolution = tf.nn.conv2d(X, filters, strides=[1,2,2,1], padding='SAME')\n",
    "'''VALID padding does not use zero-padding. SAME does'''\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(convolution, feed_dict={X:dataset})\n",
    "    \n",
    "# MAX POOLING LAYER\n",
    "X = tf.placeholder(tf.float32, shape=(None, height, width, channels))\n",
    "max_pool = tf.nn.max_pool(X, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(max_pool, feed_dict={X:: dataset})\n",
    "\n",
    "plt.imshow(output[0].astype(np.uint8)) # plots output for the 1st image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.15 Text Sequences and Time-Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.15.1 Recurrent Neural Networks (RNN) - With Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32)) # simple RNN is not great for text processing since it performs badly on long sequences\n",
    "\n",
    "# LSTM and GRU #\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU # LSTM and GRU perform considearbly better on text thatn SimpleRNN\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# For Text Sequences\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# For Oher Sequences (such as Time Series) -This adds: A Bidirectional Layer, Stacked Layers, and Dropout\n",
    "model = Sequential()\n",
    "model.add(layers.Bidirectional(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True\n",
    "                                          input_shape=(shape1, shape2, shape3)))) # for a bidirectional model (mainly NLP)\n",
    "model.add(layers.GRU(64, activation='relu', dropout=0.1, recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.15.2 1D Convolutional Neural Networks (1D CNN) - With Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for sequences, as a faster/lighter alternative to RNNs\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_lem)) # if doing a text sequence task\n",
    "model.add(layers.Conv1D(32, 7, activatoin='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalmaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(l1=1e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "# ...\n",
    "\n",
    "# 1D As Pre-processing of Long Sequences to Feed into RNNs\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 5, activation='relu', input_shape=(None, shape1, shape3)))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "movel.add(layers.GRU(32, dropout=0.1, recurrent_droppout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.16 Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.16.1 Stacked Autoencoders - Using 2 Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150  # codings\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0001\n",
    "\n",
    "def train_autoencoder(X_train, n_neurons, n_epochs, batch_size,\n",
    "                      learning_rate = 0.01, l2_reg = 0.0005, seed=42,\n",
    "                      hidden_activation=tf.nn.elu,\n",
    "                      output_activation=tf.nn.elu):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        n_inputs = X_train.shape[1]\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "        \n",
    "        my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "\n",
    "        hidden = my_dense_layer(X, n_neurons, activation=hidden_activation, name=\"hidden\")\n",
    "        outputs = my_dense_layer(hidden, n_inputs, activation=output_activation, name=\"outputs\")\n",
    "\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            n_batches = len(X_train) // batch_size\n",
    "            for iteration in range(n_batches):\n",
    "                print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                indices = rnd.permutation(len(X_train))[:batch_size]\n",
    "                X_batch = X_train[indices]\n",
    "                sess.run(training_op, feed_dict={X: X_batch})\n",
    "            loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "            print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        params = dict([(var.name, var.eval()) for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])\n",
    "        hidden_val = hidden.eval(feed_dict={X: X_train})\n",
    "        return hidden_val, params[\"hidden/kernel:0\"], params[\"hidden/bias:0\"], params[\"outputs/kernel:0\"], params[\"outputs/bias:0\"]\n",
    "\n",
    "# Train Phase 1\n",
    "hidden_output, W1, b1, W4, b4 = train_autoencoder(mnist.train.images, n_neurons=300, n_epochs=4, batch_size=150,\n",
    "                                                  output_activation=None)\n",
    "# Train Phase 2\n",
    "_, W2, b2, W3, b3 = train_autoencoder(hidden_output, n_neurons=150, n_epochs=4, batch_size=150)\n",
    "\n",
    "# Stack the Two Autoencoders\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden1 = tf.nn.elu(tf.matmul(X, W1) + b1)\n",
    "hidden2 = tf.nn.elu(tf.matmul(hidden1, W2) + b2)\n",
    "hidden3 = tf.nn.elu(tf.matmul(hidden2, W3) + b3)\n",
    "outputs = tf.matmul(hidden3, W4) + b4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.16.2 Stacked Autoencoders - Using 1 Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = tf.nn.elu\n",
    "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "weights1_init = initializer([n_inputs, n_hidden1])\n",
    "weights2_init = initializer([n_hidden1, n_hidden2])\n",
    "weights3_init = initializer([n_hidden2, n_hidden3])\n",
    "\n",
    "weights1 = tf.Variable(weights1_init, dtype=tf.float32, name=\"weights1\")\n",
    "weights2 = tf.Variable(weights2_init, dtype=tf.float32, name=\"weights2\")\n",
    "weights3 = tf.Variable(weights3_init, dtype=tf.float32, name=\"weights3\")\n",
    "\n",
    "biases1 = tf.Variable(tf.zeros(n_hidden1), name=\"biases1\")\n",
    "biases2 = tf.Variable(tf.zeros(n_hidden2), name=\"biases2\")\n",
    "biases3 = tf.Variable(tf.zeros(n_hidden3), name=\"biases3\")\n",
    "\n",
    "hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "outputs = tf.matmul(hidden2, weights3) + biases3\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "with tf.name_scope(\"phase1\"):\n",
    "    phase1_outputs = tf.matmul(hidden1, weights3) + biases3  # bypass hidden2 and hidden3\n",
    "    phase1_reconstruction_loss = tf.reduce_mean(tf.square(phase1_outputs - X))\n",
    "    phase1_reg_loss = regularizer(weights1) + regularizer(weights3)\n",
    "    phase1_loss = phase1_reconstruction_loss + phase1_reg_loss\n",
    "    phase1_training_op = optimizer.minimize(phase1_loss)\n",
    "\n",
    "with tf.name_scope(\"phase2\"):\n",
    "    phase2_reconstruction_loss = tf.reduce_mean(tf.square(hidden2 - hidden1))\n",
    "    phase2_reg_loss = regularizer(weights2) + regularizer(weights3)\n",
    "    phase2_loss = phase2_reconstruction_loss + phase2_reg_loss\n",
    "    train_vars = [weights2, biases2, weights3, biases3]\n",
    "    phase2_training_op = optimizer.minimize(phase2_loss, var_list=train_vars) # freeze hidden1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "    \n",
    "training_ops = [phase1_training_op, phase2_training_op]\n",
    "reconstruction_losses = [phase1_reconstruction_loss, phase2_reconstruction_loss]\n",
    "n_epochs = [4, 4]\n",
    "batch_sizes = [150, 150]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for phase in range(2):\n",
    "        print(\"Training phase #{}\".format(phase + 1))\n",
    "        for epoch in range(n_epochs[phase]):\n",
    "            n_batches = mnist.train.num_examples // batch_sizes[phase]\n",
    "            for iteration in range(n_batches):\n",
    "                print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_sizes[phase])\n",
    "                sess.run(training_ops[phase], feed_dict={X: X_batch})\n",
    "            loss_train = reconstruction_losses[phase].eval(feed_dict={X: X_batch})\n",
    "            print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "            saver.save(sess, \"./my_model_one_at_a_time.ckpt\")\n",
    "    loss_test = reconstruction_loss.eval(feed_dict={X: mnist.test.images})\n",
    "    print(\"Test MSE:\", loss_test)    \n",
    "    \n",
    "    \n",
    "# TYING WEIGHTS #\n",
    "'''It's easier to define the layers manually than using the dense() function\n",
    "when tying weights'''\n",
    "\n",
    "activation = tf.nn.elu\n",
    "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "weights1_init = initializer([n_inputs, n_hidden1])\n",
    "weights2_init = initializer([n_hidden1, n_hidden2])\n",
    "\n",
    "weights1 = tf.Variable(weights1_init, dtype=tf.float32, name=\"weights1\")\n",
    "weights2 = tf.Variable(weights2_init, dtype=tf.float32, name=\"weights2\")\n",
    "weights3 = tf.transpose(weights2, name=\"weights3\")  # tied weights\n",
    "weights4 = tf.transpose(weights1, name=\"weights4\")  # tied weights\n",
    "\n",
    "biases1 = tf.Variable(tf.zeros(n_hidden1), name=\"biases1\")\n",
    "biases2 = tf.Variable(tf.zeros(n_hidden2), name=\"biases2\")\n",
    "biases3 = tf.Variable(tf.zeros(n_hidden3), name=\"biases3\")\n",
    "biases4 = tf.Variable(tf.zeros(n_outputs), name=\"biases4\")\n",
    "\n",
    "hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)\n",
    "outputs = tf.matmul(hidden3, weights4) + biases4\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "reg_loss = regularizer(weights1) + regularizer(weights2)\n",
    "loss = reconstruction_loss + reg_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# UNSUPERVISED PRE-TRAINING #\n",
    "activation = tf.nn.elu\n",
    "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "weights1_init = initializer([n_inputs, n_hidden1])\n",
    "weights2_init = initializer([n_hidden1, n_hidden2])\n",
    "weights3_init = initializer([n_hidden2, n_outputs])\n",
    "\n",
    "weights1 = tf.Variable(weights1_init, dtype=tf.float32, name=\"weights1\")\n",
    "weights2 = tf.Variable(weights2_init, dtype=tf.float32, name=\"weights2\")\n",
    "weights3 = tf.Variable(weights3_init, dtype=tf.float32, name=\"weights3\")\n",
    "\n",
    "biases1 = tf.Variable(tf.zeros(n_hidden1), name=\"biases1\")\n",
    "biases2 = tf.Variable(tf.zeros(n_hidden2), name=\"biases2\")\n",
    "biases3 = tf.Variable(tf.zeros(n_outputs), name=\"biases3\")\n",
    "\n",
    "hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "logits = tf.matmul(hidden2, weights3) + biases3\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "reg_loss = regularizer(weights1) + regularizer(weights2) + regularizer(weights3)\n",
    "loss = cross_entropy + reg_loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "pretrain_saver = tf.train.Saver([weights1, weights2, biases1, biases2])\n",
    "\n",
    "# Reusing the first 2 layers of the autoencoder trained above:\n",
    "\n",
    "n_epochs = 4\n",
    "batch_size = 150\n",
    "n_labeled_instances = 20000\n",
    "\n",
    "#training_op = optimizer.minimize(loss, var_list=[weights3, biases3])  # Freeze layers 1 and 2 (optional)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    pretrain_saver.restore(sess, \"./my_model_cache_frozen.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = n_labeled_instances // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            indices = rnd.permutation(n_labeled_instances)[:batch_size]\n",
    "            X_batch, y_batch = mnist.train.images[indices], mnist.train.labels[indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train accuracy:\", accuracy_val, end=\"\\t\")\n",
    "        saver.save(sess, \"./my_model_supervised_pretrained.ckpt\")\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\"Test accuracy:\", accuracy_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.16.3 Noise Autoencoders (Dropout Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.3\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,name=\"hidden2\")                            \n",
    "hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")                            \n",
    "outputs = tf.layers.dense(hidden3, n_outputs, name=\"outputs\")        \n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, training: True})\n",
    "        loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        saver.save(sess, \"./my_model_stacked_denoising_dropout.ckpt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.16.4 Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    # Kullback Leibler divergence\n",
    "    return p * tf.log(p / q) + (1 - p) * tf.log((1 - p) / (1 - q))\n",
    "\n",
    "learning_rate = 0.01\n",
    "sparsity_target = 0.1\n",
    "sparsity_weight = 0.2\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])            # not shown in the book\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.sigmoid) # not shown\n",
    "outputs = tf.layers.dense(hidden1, n_outputs)                     # not shown\n",
    "\n",
    "hidden1_mean = tf.reduce_mean(hidden1, axis=0) # batch mean\n",
    "sparsity_loss = tf.reduce_sum(kl_divergence(sparsity_target, hidden1_mean))\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE\n",
    "loss = reconstruction_loss + sparsity_weight * sparsity_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        reconstruction_loss_val, sparsity_loss_val, loss_val = sess.run([reconstruction_loss, sparsity_loss, loss], feed_dict={X: X_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train MSE:\", reconstruction_loss_val, \"\\tSparsity loss:\", sparsity_loss_val, \"\\tTotal loss:\", loss_val)\n",
    "        saver.save(sess, \"./my_model_sparse.ckpt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.16.5 Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 500\n",
    "n_hidden2 = 500\n",
    "n_hidden3 = 20  # codings\n",
    "n_hidden4 = n_hidden2\n",
    "n_hidden5 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "learning_rate = 0.001\n",
    "\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "my_dense_layer = partial(\n",
    "    tf.layers.dense,\n",
    "    activation=tf.nn.elu,\n",
    "    kernel_initializer=initializer)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "hidden1 = my_dense_layer(X, n_hidden1)\n",
    "hidden2 = my_dense_layer(hidden1, n_hidden2)\n",
    "hidden3_mean = my_dense_layer(hidden2, n_hidden3, activation=None)\n",
    "hidden3_sigma = my_dense_layer(hidden2, n_hidden3, activation=None)\n",
    "noise = tf.random_normal(tf.shape(hidden3_sigma), dtype=tf.float32)\n",
    "hidden3 = hidden3_mean + hidden3_sigma * noise\n",
    "hidden4 = my_dense_layer(hidden3, n_hidden4)\n",
    "hidden5 = my_dense_layer(hidden4, n_hidden5)\n",
    "logits = my_dense_layer(hidden5, n_outputs, activation=None)\n",
    "outputs = tf.sigmoid(logits)\n",
    "\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits)\n",
    "reconstruction_loss = tf.reduce_sum(xentropy)\n",
    "\n",
    "eps = 1e-10 # smoothing term to avoid computing log(0) which is NaN\n",
    "latent_loss = 0.5 * tf.reduce_sum(\n",
    "    tf.square(hidden3_sigma) + tf.square(hidden3_mean)\n",
    "    - 1 - tf.log(eps + tf.square(hidden3_sigma)))\n",
    "\n",
    "loss = reconstruction_loss + latent_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        loss_val, reconstruction_loss_val, latent_loss_val = sess.run([loss, reconstruction_loss, latent_loss], feed_dict={X: X_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train total loss:\", loss_val, \"\\tReconstruction loss:\", reconstruction_loss_val, \"\\tLatent loss:\", latent_loss_val)\n",
    "        saver.save(sess, \"./my_model_variational.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Virtual Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Spark, Haddop and AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://aws.amazon.com/free/\n",
    "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html\n",
    "'''   \n",
    "# Step 1: Initiate Instance on AWS\n",
    "\n",
    "# Step 2: Set Up Putty\n",
    "\n",
    "# Step 3: Set up Spark\n",
    "    https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297\n",
    "    \n",
    "# Step 3.5: If issues with Running Jupyter notebook from Ubuntu virtual machine\n",
    "    I found out by repeatedly upgrading both Anaconda and Jupyter until there was no more update available.\n",
    "\n",
    "    $conda update conda\n",
    "    $conda update jupyter\n",
    "    Keep in mind that multiple updates may be necessary. I lost my count at 4.\n",
    "\n",
    "    During the upgrades, I noticed that openssl was also being upgraded as a dependent package of jupyter. So I thought it would be necessary to redo the openssl step in order to create a new mycert.pem using the newer version of the certificate.   \n",
    "\n",
    "    That worked and allowed me to connect to the Notebook server. I struggled a bit at first since every line of code (even a print('hello') caused an infinite loop, but that should be solvable by updating Ipykernel. That is:\n",
    "\n",
    "    $conda update ipykernel\n",
    "\n",
    "    IF NOT: go to home/ubuntu/certs and do sudo chmod 777 mycert.pem                                                                                                                  \n",
    "    IF NOT: try \"sudo /home/ubuntu/anaconda3/bin/jupyter notebook\"  \n",
    "\n",
    "    THEN run \"jupyter notebook\" from home/ubuntu/\n",
    "'''\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "%writefile example.txt\n",
    "first line\n",
    "second line\n",
    "\n",
    "textFile = sc.textFile('example.txt') # creates RDD object\n",
    "\n",
    "# Actions\n",
    "textFile.count()\n",
    "textFile.first()\n",
    "\n",
    "# Transformations\n",
    "secfind = textFile.filter(lambda line: 'second' in line)\n",
    "secfind.collect() # will return the line where second appears\n",
    "secfind.count() # returns number of lines where 'second' appears\n",
    "\n",
    "'''\n",
    "RDD - Resilient Distributed Dataset\n",
    "Transformation - Spark operation that produces an RDD\n",
    "Action - Spark operation that produces a local object\n",
    "Job - Sequence of transformations on data with a final action (all three above combined)\n",
    "\n",
    "'''\n",
    "\n",
    "# EX:\n",
    "# Grab (State, Amount)\n",
    "step1 = clean.map(lambda lst: (lst[3], lst[-1]))\n",
    "# Reduce by Key\n",
    "step2 = step1.reduceByKey(lambda amt1, amt2: float(amt1) + float(amt2))\n",
    "# Get rid of State, Amount titles\n",
    "step3 = step2.filter(lambda x:not x[0] == 'State')\n",
    "# Sort Results by the amount\n",
    "step4 = step3.sortBy(lambda stAmount: stAmount[1], ascending=False)\n",
    "# Action\n",
    "step4.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
